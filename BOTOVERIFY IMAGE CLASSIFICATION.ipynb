{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "keras\n",
    "tensorflow\n",
    "pillow\n",
    "scikit-image\n",
    "h5py\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15323 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare an iterators for each dataset\n",
    "datagen = ImageDataGenerator()\n",
    "data = datagen.flow_from_directory('train/', class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bad background images 1690\n",
      "Total number of blurry images 700\n",
      "Total number of face in background images 89\n",
      "Total number of good images 6240\n",
      "Total number of invalid passport images 786\n",
      "Total number of human faces in bg images 0\n",
      "Total number of staple defaced images 10\n"
     ]
    }
   ],
   "source": [
    "totalBadBG = len([name for name in os.listdir('train/Bad_Backgroud') if os.path.isfile(os.path.join('train/Bad_Backgroud', name))])\n",
    "totalBlurry = len([name for name in os.listdir('train/Blurry') if os.path.isfile(os.path.join('train/Blurry', name))])\n",
    "totalFaceBG = len([name for name in os.listdir('train/Face_in_Background') if os.path.isfile(os.path.join('train/Face_in_Background', name))])\n",
    "totalGood = len([name for name in os.listdir('train/Good_Images') if os.path.isfile(os.path.join('train/Good_Images', name))])\n",
    "totalInvalidP = len([name for name in os.listdir('train/Invalid_Passport') if os.path.isfile(os.path.join('train/Invalid_Passport', name))])\n",
    "totalNoHumanFace = len([name for name in os.listdir('train/No_Human_Face') if os.path.isfile(os.path.join('train/No_Human_Face', name))])\n",
    "totalStapleDefaced = len([name for name in os.listdir('train/Staple_Defaced') if os.path.isfile(os.path.join('train/Staple_Defaced', name))])\n",
    "\n",
    "print(f'Total number of bad background images {totalBadBG}')\n",
    "print(f'Total number of blurry images {totalBlurry}')\n",
    "print(f'Total number of face in background images {totalFaceBG}')\n",
    "print(f'Total number of good images {totalGood}')\n",
    "print(f'Total number of invalid passport images {totalInvalidP}')\n",
    "print(f'Total number of human faces in bg images {totalNoHumanFace}')\n",
    "print(f'Total number of staple defaced images {totalStapleDefaced}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12262 images belonging to 7 classes.\n",
      "Found 3061 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"train/\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    \"train/\", # same directory as training data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "=================================================================\n",
      "Total params: 2,915,648\n",
      "Trainable params: 2,915,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/notebook-5.7.8/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"bl...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import models\n",
    "model1  = Model(inputs = model.input, output = model.get_layer('block4_conv1').output)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Model)              (None, 28, 28, 512)       2915648   \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 128)       589952    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 3,514,311\n",
      "Trainable params: 3,514,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model1 = models.Sequential()\n",
    "Model1.add(model1)\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "Model1.add(Conv2D(128, (3,3), activation='relu', padding = 'same'))\n",
    "Model1.add(MaxPooling2D((2,2), padding = 'same'))\n",
    "Model1.add(GlobalAveragePooling2D())\n",
    "Model1.add(Dense(64, activation = 'relu'))\n",
    "Model1.add(Dense(7, activation = 'softmax'))\n",
    "Model1.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "Model1.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Model)              (None, 28, 28, 512)       2915648   \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 128)       589952    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 3,514,311\n",
      "Trainable params: 3,514,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/notebook-5.7.8/lib/python3.7/site-packages/PIL/Image.py\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/notebook-5.7.8/lib/python3.7/site-packages/PIL/Image.py\n"
     ]
    }
   ],
   "source": [
    "import Image\n",
    "print(Image.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint_callback = ModelCheckpoint(\"model2\"+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint_callback = ModelCheckpoint(\"model1\"+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "32/32 [==============================] - 1187s 37s/step - loss: 1.8802 - accuracy: 0.7192 - val_loss: 0.7671 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76711, saving model to model1.h5\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 1149s 36s/step - loss: 0.7630 - accuracy: 0.7933 - val_loss: 0.8731 - val_accuracy: 0.7637\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.76711\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 1136s 35s/step - loss: 0.7536 - accuracy: 0.7646 - val_loss: 0.6796 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76711 to 0.67958, saving model to model1.h5\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 1149s 36s/step - loss: 0.5928 - accuracy: 0.8081 - val_loss: 0.4012 - val_accuracy: 0.8145\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67958 to 0.40122, saving model to model1.h5\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 1148s 36s/step - loss: 0.5280 - accuracy: 0.8276 - val_loss: 0.4735 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.40122\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 1136s 35s/step - loss: 0.4227 - accuracy: 0.8604 - val_loss: 0.5094 - val_accuracy: 0.8703\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.40122\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 1261s 39s/step - loss: 0.4820 - accuracy: 0.8447 - val_loss: 0.5947 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.40122\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 1252s 39s/step - loss: 0.4286 - accuracy: 0.8657 - val_loss: 0.3326 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.40122 to 0.33261, saving model to model1.h5\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - 1212s 38s/step - loss: 0.3737 - accuracy: 0.8769 - val_loss: 0.2818 - val_accuracy: 0.8828\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33261 to 0.28177, saving model to model1.h5\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - 1223s 38s/step - loss: 0.3347 - accuracy: 0.8960 - val_loss: 0.4488 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.28177\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - 1226s 38s/step - loss: 0.3389 - accuracy: 0.8945 - val_loss: 0.1317 - val_accuracy: 0.9414\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.28177 to 0.13169, saving model to model1.h5\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - 1214s 38s/step - loss: 0.3740 - accuracy: 0.8828 - val_loss: 0.2378 - val_accuracy: 0.9062\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.13169\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - 1197s 37s/step - loss: 0.2997 - accuracy: 0.9038 - val_loss: 0.3449 - val_accuracy: 0.8984\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.13169\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - 1229s 38s/step - loss: 0.3036 - accuracy: 0.9048 - val_loss: 0.5032 - val_accuracy: 0.9102\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.13169\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - 1198s 37s/step - loss: 0.2816 - accuracy: 0.9136 - val_loss: 0.2969 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.13169\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - 1197s 37s/step - loss: 0.2869 - accuracy: 0.9141 - val_loss: 0.1148 - val_accuracy: 0.9219\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13169 to 0.11477, saving model to model1.h5\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - 1204s 38s/step - loss: 0.2227 - accuracy: 0.9243 - val_loss: 0.2242 - val_accuracy: 0.9492\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11477\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - 1158s 36s/step - loss: 0.2454 - accuracy: 0.9283 - val_loss: 0.2985 - val_accuracy: 0.9381\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11477\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - 1187s 37s/step - loss: 0.2311 - accuracy: 0.9287 - val_loss: 0.2823 - val_accuracy: 0.9121\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11477\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - 1192s 37s/step - loss: 0.2460 - accuracy: 0.9287 - val_loss: 0.1454 - val_accuracy: 0.9395\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11477\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - 1161s 36s/step - loss: 0.2600 - accuracy: 0.9263 - val_loss: 0.1252 - val_accuracy: 0.9473\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x140256a50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1.fit_generator(train_generator, steps_per_epoch=32, epochs = 40, validation_data=validation_generator, validation_steps=8, callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def fix_layer0(filename, batch_input_shape, dtype):\n",
    "    import h5py\n",
    "    import json\n",
    "    \n",
    "    with h5py.File(filename, 'r+') as f:\n",
    "        model_config = json.loads(f.attrs['model_config'].decode('utf-8'))\n",
    "        layer0 = model_config['config']['layers'][0]['config']\n",
    "        layer0['batch_input_shape'] = batch_input_shape\n",
    "        layer0['dtype'] = dtype\n",
    "        f.attrs['model_config'] = json.dumps(model_config).encode('utf-8')\n",
    "\n",
    "fix_layer0('model.h5', [None, 224, 224, 3], 'float32')\n",
    "\n",
    "loaded_model = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE PREDICTIONS PER FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 213 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_generator = test_gen.flow_from_directory(\n",
    "        \"test/\",\n",
    "        target_size=(224, 224),\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "\n",
    "def load(filename):\n",
    "    np_image = Image.open(filename)\n",
    "    np_image = np.array(np_image).astype('float32')/255\n",
    "    np_image = transform.resize(np_image, (224, 224, 3))\n",
    "    np_image = np.expand_dims(np_image, axis=0)\n",
    "    return np_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startPrediction(stateName):\n",
    "    import os\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    test_gen = ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    currentList = f'{stateName}/'\n",
    "    directoryList = os.listdir(currentList)\n",
    "    #loop through the centers\n",
    "    current = 0\n",
    "    total = len([name for name in os.listdir(currentList) if os.path.isfile(os.path.join(currentList, name))])\n",
    "    total = sum([len(files) for r, d, files in os.walk(currentList)])\n",
    "    for directory in directoryList:\n",
    "        print(directory)\n",
    "        if directory != '.DS_Store':\n",
    "            currentImageDirectory = f'{currentList}{directory}'\n",
    "            for subdir, dirs, files in os.walk(currentImageDirectory):\n",
    "                print(currentImageDirectory)\n",
    "                \n",
    "                for file in files:\n",
    "                    filepath = subdir + os.sep + file\n",
    "                    if filepath.endswith(\".jpg\"):\n",
    "                        image = load(filepath)\n",
    "                        _class = Model1.predict_classes(image)\n",
    "                        if _class == 0:\n",
    "                            #BAD BACKGROUND\n",
    "                            if os.path.exists(f\"{currentImageDirectory}/BAD_BACKGROUND\") == True:\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/BAD_BACKGROUND\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            else:\n",
    "                                #make directory\n",
    "                                os.mkdir(f\"{currentImageDirectory}/BAD_BACKGROUND\")\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/BAD_BACKGROUND\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            current +=1\n",
    "                                    \n",
    "                        elif _class == 1:\n",
    "                            #BLURRY\n",
    "                            if os.path.exists(f\"{currentImageDirectory}/BLURRY\") == True:\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/BLURRY\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            else:\n",
    "                                #make directory\n",
    "                                os.mkdir(f\"{currentImageDirectory}/BLURRY\")\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/BLURRY\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            current +=1\n",
    "                                    \n",
    "                        elif _class == 2:\n",
    "                            #FACE_IN_BACKGROUND\n",
    "                            if os.path.exists(f\"{currentImageDirectory}/FACE_IN_BACKGROUND\") == True:\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/FACE_IN_BACKGROUND\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            else:\n",
    "                                #make directory\n",
    "                                os.mkdir(f\"{currentImageDirectory}/FACE_IN_BACKGROUND\")\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/FACE_IN_BACKGROUND\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            current +=1\n",
    "                                    \n",
    "                        elif _class == 3:\n",
    "                            #GOOD_IMAGES\n",
    "                            if os.path.exists(f\"{currentImageDirectory}/GOOD_IMAGES\") == True:\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/GOOD_IMAGES\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            else:\n",
    "                                #make directory\n",
    "                                os.mkdir(f\"{currentImageDirectory}/GOOD_IMAGES\")\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/GOOD_IMAGES\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            current +=1\n",
    "                                    \n",
    "                        elif _class == 4:\n",
    "                            #INVALID_PASSPORT\n",
    "                            if os.path.exists(f\"{currentImageDirectory}/INVALID_PASSPORT\") == True:\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/INVALID_PASSPORT\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            else:\n",
    "                                #make directory\n",
    "                                os.mkdir(f\"{currentImageDirectory}/INVALID_PASSPORT\")\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/INVALID_PASSPORT\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            current +=1\n",
    "                                    \n",
    "                        elif _class == 5:\n",
    "                            #NO_HUMAN_FACE\n",
    "                            if os.path.exists(f\"{currentImageDirectory}/NO_HUMAN_FACE\") == True:\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/NO_HUMAN_FACE\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            else:\n",
    "                                #make directory\n",
    "                                os.mkdir(f\"{currentImageDirectory}/NO_HUMAN_FACE\")\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/NO_HUMAN_FACE\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            current +=1\n",
    "                                    \n",
    "                        elif _class == 6:\n",
    "                            #STAPLED FACE\n",
    "                            if os.path.exists(f\"{currentImageDirectory}/STAPLED_FACE\") == True:\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/STAPLED_FACE\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            else:\n",
    "                                #make directory\n",
    "                                os.mkdir(f\"{currentImageDirectory}/STAPLED_FACE\")\n",
    "                                try:\n",
    "                                    shutil.move(f\"{filepath}\" , f\"{currentImageDirectory}/STAPLED_FACE\")\n",
    "                                except:\n",
    "                                    error = 1\n",
    "                            current +=1\n",
    "                        \n",
    "                        print(f'{current} ---of--- {total}')\n",
    "                              \n",
    "                        \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4380303\n",
      "438/4380303\n",
      "438/4380303\n",
      "1 ---of--- 214\n",
      "2 ---of--- 214\n",
      "3 ---of--- 214\n",
      "4 ---of--- 214\n",
      "5 ---of--- 214\n",
      "6 ---of--- 214\n",
      "7 ---of--- 214\n",
      "8 ---of--- 214\n",
      "9 ---of--- 214\n",
      "10 ---of--- 214\n",
      "11 ---of--- 214\n",
      "12 ---of--- 214\n",
      "13 ---of--- 214\n",
      "14 ---of--- 214\n",
      "15 ---of--- 214\n",
      "16 ---of--- 214\n",
      ".DS_Store\n",
      "4380304\n",
      "438/4380304\n",
      "438/4380304\n",
      "17 ---of--- 214\n",
      "18 ---of--- 214\n",
      "19 ---of--- 214\n",
      "20 ---of--- 214\n",
      "21 ---of--- 214\n",
      "22 ---of--- 214\n",
      "23 ---of--- 214\n",
      "24 ---of--- 214\n",
      "25 ---of--- 214\n",
      "26 ---of--- 214\n",
      "27 ---of--- 214\n",
      "28 ---of--- 214\n",
      "29 ---of--- 214\n",
      "30 ---of--- 214\n",
      "31 ---of--- 214\n",
      "32 ---of--- 214\n",
      "33 ---of--- 214\n",
      "34 ---of--- 214\n",
      "35 ---of--- 214\n",
      "36 ---of--- 214\n",
      "37 ---of--- 214\n",
      "38 ---of--- 214\n",
      "39 ---of--- 214\n",
      "40 ---of--- 214\n",
      "41 ---of--- 214\n",
      "42 ---of--- 214\n",
      "43 ---of--- 214\n",
      "44 ---of--- 214\n",
      "45 ---of--- 214\n",
      "46 ---of--- 214\n",
      "47 ---of--- 214\n",
      "48 ---of--- 214\n",
      "49 ---of--- 214\n",
      "50 ---of--- 214\n",
      "51 ---of--- 214\n",
      "52 ---of--- 214\n",
      "53 ---of--- 214\n",
      "4380305\n",
      "438/4380305\n",
      "438/4380305\n",
      "54 ---of--- 214\n",
      "55 ---of--- 214\n",
      "56 ---of--- 214\n",
      "57 ---of--- 214\n",
      "58 ---of--- 214\n",
      "59 ---of--- 214\n",
      "60 ---of--- 214\n",
      "61 ---of--- 214\n",
      "62 ---of--- 214\n",
      "63 ---of--- 214\n",
      "64 ---of--- 214\n",
      "65 ---of--- 214\n",
      "66 ---of--- 214\n",
      "67 ---of--- 214\n",
      "68 ---of--- 214\n",
      "69 ---of--- 214\n",
      "70 ---of--- 214\n",
      "71 ---of--- 214\n",
      "72 ---of--- 214\n",
      "73 ---of--- 214\n",
      "74 ---of--- 214\n",
      "75 ---of--- 214\n",
      "76 ---of--- 214\n",
      "77 ---of--- 214\n",
      "78 ---of--- 214\n",
      "79 ---of--- 214\n",
      "80 ---of--- 214\n",
      "4380302\n",
      "438/4380302\n",
      "438/4380302\n",
      "81 ---of--- 214\n",
      "82 ---of--- 214\n",
      "83 ---of--- 214\n",
      "84 ---of--- 214\n",
      "85 ---of--- 214\n",
      "86 ---of--- 214\n",
      "87 ---of--- 214\n",
      "88 ---of--- 214\n",
      "89 ---of--- 214\n",
      "90 ---of--- 214\n",
      "91 ---of--- 214\n",
      "92 ---of--- 214\n",
      "93 ---of--- 214\n",
      "94 ---of--- 214\n",
      "95 ---of--- 214\n",
      "96 ---of--- 214\n",
      "97 ---of--- 214\n",
      "4380101\n",
      "438/4380101\n",
      "438/4380101\n",
      "98 ---of--- 214\n",
      "99 ---of--- 214\n",
      "100 ---of--- 214\n",
      "101 ---of--- 214\n",
      "102 ---of--- 214\n",
      "103 ---of--- 214\n",
      "104 ---of--- 214\n",
      "105 ---of--- 214\n",
      "106 ---of--- 214\n",
      "107 ---of--- 214\n",
      "108 ---of--- 214\n",
      "109 ---of--- 214\n",
      "110 ---of--- 214\n",
      "111 ---of--- 214\n",
      "4380201\n",
      "438/4380201\n",
      "438/4380201\n",
      "112 ---of--- 214\n",
      "113 ---of--- 214\n",
      "438/4380201\n",
      "114 ---of--- 214\n",
      "115 ---of--- 214\n",
      "116 ---of--- 214\n",
      "117 ---of--- 214\n",
      "118 ---of--- 214\n",
      "119 ---of--- 214\n",
      "120 ---of--- 214\n",
      "121 ---of--- 214\n",
      "122 ---of--- 214\n",
      "123 ---of--- 214\n",
      "124 ---of--- 214\n",
      "125 ---of--- 214\n",
      "126 ---of--- 214\n",
      "127 ---of--- 214\n",
      "128 ---of--- 214\n",
      "129 ---of--- 214\n",
      "130 ---of--- 214\n",
      "131 ---of--- 214\n",
      "132 ---of--- 214\n",
      "133 ---of--- 214\n",
      "134 ---of--- 214\n",
      "135 ---of--- 214\n",
      "136 ---of--- 214\n",
      "137 ---of--- 214\n",
      "138 ---of--- 214\n",
      "139 ---of--- 214\n",
      "140 ---of--- 214\n",
      "141 ---of--- 214\n",
      "142 ---of--- 214\n",
      "143 ---of--- 214\n",
      "144 ---of--- 214\n",
      "145 ---of--- 214\n",
      "146 ---of--- 214\n",
      "147 ---of--- 214\n",
      "148 ---of--- 214\n",
      "149 ---of--- 214\n",
      "150 ---of--- 214\n",
      "151 ---of--- 214\n",
      "152 ---of--- 214\n",
      "153 ---of--- 214\n",
      "154 ---of--- 214\n",
      "155 ---of--- 214\n",
      "156 ---of--- 214\n",
      "157 ---of--- 214\n",
      "158 ---of--- 214\n",
      "159 ---of--- 214\n",
      "160 ---of--- 214\n",
      "161 ---of--- 214\n",
      "162 ---of--- 214\n",
      "163 ---of--- 214\n",
      "164 ---of--- 214\n",
      "165 ---of--- 214\n",
      "166 ---of--- 214\n",
      "4380301\n",
      "438/4380301\n",
      "438/4380301\n",
      "167 ---of--- 214\n",
      "168 ---of--- 214\n",
      "169 ---of--- 214\n",
      "170 ---of--- 214\n",
      "171 ---of--- 214\n",
      "172 ---of--- 214\n",
      "173 ---of--- 214\n",
      "174 ---of--- 214\n",
      "175 ---of--- 214\n",
      "176 ---of--- 214\n",
      "177 ---of--- 214\n",
      "178 ---of--- 214\n",
      "179 ---of--- 214\n",
      "180 ---of--- 214\n",
      "181 ---of--- 214\n",
      "182 ---of--- 214\n",
      "183 ---of--- 214\n",
      "184 ---of--- 214\n",
      "185 ---of--- 214\n",
      "186 ---of--- 214\n",
      "187 ---of--- 214\n",
      "188 ---of--- 214\n",
      "189 ---of--- 214\n",
      "190 ---of--- 214\n",
      "191 ---of--- 214\n",
      "192 ---of--- 214\n",
      "193 ---of--- 214\n",
      "194 ---of--- 214\n",
      "195 ---of--- 214\n",
      "4380102\n",
      "438/4380102\n",
      "438/4380102\n",
      "196 ---of--- 214\n",
      "197 ---of--- 214\n",
      "198 ---of--- 214\n",
      "199 ---of--- 214\n",
      "200 ---of--- 214\n",
      "201 ---of--- 214\n",
      "202 ---of--- 214\n",
      "203 ---of--- 214\n",
      "204 ---of--- 214\n",
      "205 ---of--- 214\n",
      "206 ---of--- 214\n",
      "207 ---of--- 214\n",
      "208 ---of--- 214\n",
      "209 ---of--- 214\n",
      "210 ---of--- 214\n",
      "211 ---of--- 214\n",
      "212 ---of--- 214\n"
     ]
    }
   ],
   "source": [
    "startPrediction('438')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT ONE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_generator = test_gen.flow_from_directory(\n",
    "        \"test/\",\n",
    "        target_size=(224, 224),\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        batch_size=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = loaded_model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "array  = np.array(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9596351e-01, 3.0768332e-01, 6.7714512e-02, 2.9282017e-02,\n",
       "        6.9323264e-02, 3.8860162e-04, 2.9644707e-02]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9350958e-01, 1.0001611e-02, 2.0744037e-02, 7.2890091e-01,\n",
       "        4.6525884e-02, 3.8137823e-05, 2.7990725e-04]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1.predict_classes(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/notebook-5.7.8/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "img_height,img_width = 64,64\n",
    "num_classes = 6\n",
    "#If imagenet weights are being loaded, \n",
    "#input must have a static square shape (one of (128, 128), (160, 160), (192, 192), or (224, 224))\n",
    "base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (img_height,img_width,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12262 images belonging to 7 classes.\n",
      "Found 3061 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"train/\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    \"train/\", # same directory as training data\n",
    "    target_size=(64, 64),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "predictions = Dense(7, activation= 'softmax')(x)\n",
    "model = Model(inputs = base_model.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "# sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 151s 5s/step - loss: 1.1856 - accuracy: 0.6475 - val_loss: 16.0457 - val_accuracy: 0.7832\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.11477\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 128s 4s/step - loss: 0.6329 - accuracy: 0.8136 - val_loss: 1.2818 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.11477\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 126s 4s/step - loss: 0.5285 - accuracy: 0.8403 - val_loss: 0.9815 - val_accuracy: 0.6973\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11477\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 140s 4s/step - loss: 0.5000 - accuracy: 0.8516 - val_loss: 0.6192 - val_accuracy: 0.8379\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11477\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 155s 5s/step - loss: 0.4805 - accuracy: 0.8716 - val_loss: 0.5409 - val_accuracy: 0.8281\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11477\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 122s 4s/step - loss: 0.3966 - accuracy: 0.8867 - val_loss: 0.3657 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11477\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 152s 5s/step - loss: 0.4684 - accuracy: 0.8765 - val_loss: 0.5539 - val_accuracy: 0.8359\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11477\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 126s 4s/step - loss: 0.3610 - accuracy: 0.8922 - val_loss: 0.4872 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11477\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 163s 5s/step - loss: 0.3580 - accuracy: 0.8975 - val_loss: 0.3705 - val_accuracy: 0.8789\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11477\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 157s 5s/step - loss: 0.4310 - accuracy: 0.8857 - val_loss: 0.3492 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11477\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 158s 5s/step - loss: 0.3949 - accuracy: 0.8877 - val_loss: 0.9050 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11477\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 153s 5s/step - loss: 0.3778 - accuracy: 0.8984 - val_loss: 0.4860 - val_accuracy: 0.9102\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11477\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 121s 4s/step - loss: 0.3923 - accuracy: 0.8862 - val_loss: 0.2948 - val_accuracy: 0.9160\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11477\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 120s 4s/step - loss: 0.3389 - accuracy: 0.9009 - val_loss: 0.2501 - val_accuracy: 0.9219\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11477\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 138s 4s/step - loss: 0.3216 - accuracy: 0.9028 - val_loss: 0.1087 - val_accuracy: 0.9121\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11477 to 0.10872, saving model to model1.h5\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 141s 4s/step - loss: 0.3753 - accuracy: 0.9055 - val_loss: 0.2082 - val_accuracy: 0.9121\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.10872\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 139s 4s/step - loss: 0.2939 - accuracy: 0.9136 - val_loss: 0.2464 - val_accuracy: 0.9023\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.10872\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 130s 4s/step - loss: 0.3281 - accuracy: 0.9092 - val_loss: 0.3746 - val_accuracy: 0.9202\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.10872\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 135s 4s/step - loss: 0.3308 - accuracy: 0.9019 - val_loss: 0.2954 - val_accuracy: 0.9258\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.10872\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 137s 4s/step - loss: 0.2958 - accuracy: 0.9141 - val_loss: 0.2132 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.10872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x189f8f8d0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=32, epochs = 50, validation_data=validation_generator, validation_steps=8, callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2 WITH DEEPER NETWROKS IN VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12262 images belonging to 7 classes.\n",
      "Found 3061 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"train/\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    \"train/\", # same directory as training data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D, InputLayer\n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/notebook-5.7.8/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import models\n",
    "model2  = Model(inputs = model.input, output = model.get_layer('fc2').output)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_2 (Model)              (None, 4096)              134260544 \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 28679     \n",
      "=================================================================\n",
      "Total params: 134,289,223\n",
      "Trainable params: 134,289,223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model2 = models.Sequential()\n",
    "Model2.add(model2)\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "Model2.add(Dense(7, activation = 'softmax'))\n",
    "Model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "Model2.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/32 [==============================] - 2069s 65s/step - loss: 10.2417 - accuracy: 0.7178 - val_loss: 1.0631 - val_accuracy: 0.7793\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.06309, saving model to model1.h5\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 1807s 56s/step - loss: 0.8036 - accuracy: 0.7852 - val_loss: 1.0900 - val_accuracy: 0.7910\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.06309\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 1892s 59s/step - loss: 0.7816 - accuracy: 0.7759 - val_loss: 0.4071 - val_accuracy: 0.8125\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.06309 to 0.40714, saving model to model1.h5\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 1850s 58s/step - loss: 0.5526 - accuracy: 0.8279 - val_loss: 0.3057 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.40714 to 0.30573, saving model to model1.h5\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 1736s 54s/step - loss: 0.3670 - accuracy: 0.9038 - val_loss: 0.3761 - val_accuracy: 0.9121\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30573\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 1914s 60s/step - loss: 0.3131 - accuracy: 0.9087 - val_loss: 0.1197 - val_accuracy: 0.9182\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30573 to 0.11974, saving model to model1.h5\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 1890s 59s/step - loss: 0.3163 - accuracy: 0.9077 - val_loss: 0.2950 - val_accuracy: 0.8945\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11974\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 1758s 55s/step - loss: 0.3640 - accuracy: 0.8922 - val_loss: 0.4887 - val_accuracy: 0.8203\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11974\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 1841s 58s/step - loss: 0.3447 - accuracy: 0.9033 - val_loss: 0.2123 - val_accuracy: 0.9180\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11974\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 1965s 61s/step - loss: 0.2865 - accuracy: 0.9150 - val_loss: 0.4308 - val_accuracy: 0.9141\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11974\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 1969s 62s/step - loss: 0.2947 - accuracy: 0.9175 - val_loss: 0.2340 - val_accuracy: 0.9316\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x147f02750>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model2.fit_generator(train_generator, steps_per_epoch=32, epochs = 30, validation_data=validation_generator, validation_steps=8, callbacks=[early_stopping_callback, checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model2.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
